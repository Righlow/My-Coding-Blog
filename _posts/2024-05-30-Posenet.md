---
title: "Posenet"
date: 2024-05-30
# technique 1
---
## Teahchable machine
# Variation: Volume

<img src="/My-Coding-Blog/images/volume-down.png" alt="some text" width="80%">

In this variation i started by defining my three classes, first one i created handles 'volume up' and second for decreasing volume and the third one keeps track of no action.If the left wrist is raised above the midpoint of the canvas the volume will increase by calling the method volumeup.If the right hand is raised above the midpoint of the canvas volume will decrease.This model displays a clear approach to volume control handling and it used in many practical situations

<img src="/My-Coding-Blog/images/volume-up.png" alt="some text" width="80%">


<img src="/My-Coding-Blog/images/none.png" alt="some text" width="80%">




Below is the generated code that was trained in the model.

```

<div>Teachable Machine Pose Model</div>
<button type="button" onclick="init()">Start</button>
<div><canvas id="canvas"></canvas></div>
<div id="label-container"></div>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/pose@0.8/dist/teachablemachine-pose.min.js"></script>
<script type="text/javascript">
    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/pose

    // the link to your model provided by Teachable Machine export panel
    const URL = "./my_model/";
    let model, webcam, ctx, labelContainer, maxPredictions;

    async function init() {
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";

        // load the model and metadata
        // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
        // Note: the pose library adds a tmPose object to your window (window.tmPose)
        model = await tmPose.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();

        // Convenience function to setup a webcam
        const size = 200;
        const flip = true; // whether to flip the webcam
        webcam = new tmPose.Webcam(size, size, flip); // width, height, flip
        await webcam.setup(); // request access to the webcam
        await webcam.play();
        window.requestAnimationFrame(loop);

        // append/get elements to the DOM
        const canvas = document.getElementById("canvas");
        canvas.width = size; canvas.height = size;
        ctx = canvas.getContext("2d");
        labelContainer = document.getElementById("label-container");
        for (let i = 0; i < maxPredictions; i++) { // and class labels
            labelContainer.appendChild(document.createElement("div"));
        }
    }

    async function loop(timestamp) {
        webcam.update(); // update the webcam frame
        await predict();
        window.requestAnimationFrame(loop);
    }

    async function predict() {
        // Prediction #1: run input through posenet
        // estimatePose can take in an image, video or canvas html element
        const { pose, posenetOutput } = await model.estimatePose(webcam.canvas);
        // Prediction 2: run input through teachable machine classification model
        const prediction = await model.predict(posenetOutput);

        for (let i = 0; i < maxPredictions; i++) {
            const classPrediction =
                prediction[i].className + ": " + prediction[i].probability.toFixed(2);
            labelContainer.childNodes[i].innerHTML = classPrediction;
        }

        // finally draw the poses
        drawPose(pose);
    }

    function drawPose(pose) {
        if (webcam.canvas) {
            ctx.drawImage(webcam.canvas, 0, 0);
            // draw the keypoints and skeleton
            if (pose) {
                const minPartConfidence = 0.5;
                tmPose.drawKeypoints(pose.keypoints, minPartConfidence, ctx);
                tmPose.drawSkeleton(pose.keypoints, minPartConfidence, ctx);
            }
        }
    }
</script>
```


# Variation: Two
# Web Socket

```
 let connection = new WebSocket('wss://cco6006-tomorrows-web-test.glitch.me/');

let dataPoints=[];

let myAppID="DavesApp"
let isConnected=false

connection.onopen = function () {
    console.log("I am connected to the server");
  isConnected=true
};

connection.onerror = function (error) {
    console.log('WebSocket Error ', error);
};

function tryParseJSONObject (jsonString){
    try {
        var o = JSON.parse(jsonString);
        if (o && typeof o === "object") {
            return o;
        }
    }
    catch (e) { }

    return false;
};

connection.onmessage = function (e) {
    let parsed=tryParseJSONObject(e.data);
    // console.log('I received: ', parsed?parsed:e.data);
    if(parsed.appID && parsed.appID==myAppID){
        // console.log("This data is for me")
      processIncomingData(parsed.data)
    } else {
        // console.log("This data is for someone else")
      
    }
};

function sendData(data){
    let message={
        appID: myAppID,
        data: data
    }
    // console.log('I am sending :'+message);
    if(isConnected){
      connection.send(JSON.stringify(message));
    }
}
```

This is a variation we went through in class recently, the sections of code below covers  websocket management.From my understandin a web socket managemnt involes the communication between a client and a server in real life time. A real life example where this would be extremely useful is Live chat applications.With regards to the demo we did in class we firstly established a connection the url server.Secondly created our variables 'data point' , 'Daves app' which will application instance and the 'isConnected' indicates connection status.The code includes event handlers for connection staus.

# Variation Three

<img src="/My-Coding-Blog/images/Car-rear.png" alt=" Car rear model" width="80%">

in this third variation i defined four diffrent class which count for the four possible cases for the rear of a car.A case for opening the rear,closing and for locking and unlocking.using the teachable machine generator.with regards to the opening the rear, once the machine detects that both handas and arms are up the rear will automatically open up this is for if the human is holding something with both arms.The rear will close if it detects both arms down.The rear will only lock if detects both arms and hands with an object in one of the hands.The rear will lock once it detects both hands holding the object which is supposed to be the key. 


<img src="/My-Coding-Blog/images/rear1.png" alt="case one" width="80%">

<img src="/My-Coding-Blog/images/rear2.png" alt="case 2" width="80%">

<img src="/My-Coding-Blog/images/rear4.png" alt="case4" width="80%">

Below is the code which was trained for this model.

```
 <div>Teachable Machine Pose Model</div>
<button type="button" onclick="init()">Start</button>
<div><canvas id="canvas"></canvas></div>
<div id="label-container"></div>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/pose@0.8/dist/teachablemachine-pose.min.js"></script>
<script type="text/javascript">
    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/pose

    // the link to your model provided by Teachable Machine export panel
    const URL = "./my_model/";
    let model, webcam, ctx, labelContainer, maxPredictions;

    async function init() {
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";

        // load the model and metadata
        // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
        // Note: the pose library adds a tmPose object to your window (window.tmPose)
        model = await tmPose.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();

        // Convenience function to setup a webcam
        const size = 200;
        const flip = true; // whether to flip the webcam
        webcam = new tmPose.Webcam(size, size, flip); // width, height, flip
        await webcam.setup(); // request access to the webcam
        await webcam.play();
        window.requestAnimationFrame(loop);

        // append/get elements to the DOM
        const canvas = document.getElementById("canvas");
        canvas.width = size; canvas.height = size;
        ctx = canvas.getContext("2d");
        labelContainer = document.getElementById("label-container");
        for (let i = 0; i < maxPredictions; i++) { // and class labels
            labelContainer.appendChild(document.createElement("div"));
        }
    }

    async function loop(timestamp) {
        webcam.update(); // update the webcam frame
        await predict();
        window.requestAnimationFrame(loop);
    }

    async function predict() {
        // Prediction #1: run input through posenet
        // estimatePose can take in an image, video or canvas html element
        const { pose, posenetOutput } = await model.estimatePose(webcam.canvas);
        // Prediction 2: run input through teachable machine classification model
        const prediction = await model.predict(posenetOutput);

        for (let i = 0; i < maxPredictions; i++) {
            const classPrediction =
                prediction[i].className + ": " + prediction[i].probability.toFixed(2);
            labelContainer.childNodes[i].innerHTML = classPrediction;
        }

        // finally draw the poses
        drawPose(pose);
    }

    function drawPose(pose) {
        if (webcam.canvas) {
            ctx.drawImage(webcam.canvas, 0, 0);
            // draw the keypoints and skeleton
            if (pose) {
                const minPartConfidence = 0.5;
                tmPose.drawKeypoints(pose.keypoints, minPartConfidence, ctx);
                tmPose.drawSkeleton(pose.keypoints, minPartConfidence, ctx);
            }
        }
    }
</script>
```








